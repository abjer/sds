{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 19: Tools for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Today you will be doing a few exercises with parallel processing using Python's `multiprocessing` package, and a couple others that bends your mind into thinking inside the MapReduce paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.1.1**: Explain the code from my lecture. Put a comment above each line of code and explain what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 10, 18, 28, 40, 54, 70, 88, 108]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def worker(x):\n",
    "    return x * (x + 3)\n",
    "\n",
    "p = Pool(4)  # <-- what does number mean?\n",
    "\n",
    "inputs = range(10)\n",
    "outputs = []\n",
    "\n",
    "for result in p.imap(worker, inputs):  # <-- why `p.imap` and not just `p.map`?\n",
    "    outputs.append(result)\n",
    "    \n",
    "p.close()\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.1.2**: Do the same as above in serial, with a single line of code.<br>\n",
    "> *Hint: Use a list comprehension*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.1.3**: Take the following bit of code and parallelize it. Report the speedup (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.055428\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "iris = datasets.load_iris().data\n",
    "\n",
    "start = dt.now()\n",
    "\n",
    "pairwise_distances = []\n",
    "for i in range(iris.shape[0]):\n",
    "    for j in range(i+1, iris.shape[0]):\n",
    "        pairwise_distances.append(np.sqrt(sum((iris[i] - iris[j])**2)))\n",
    "\n",
    "print(\"Time:\", (dt.now() - start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Running your first MapReduce job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get started right, lets make sure you're all setup with `mrjob` (you should be able to install it with `conda`, otherwise try `pip`), so you can write MapReduce jobs in Python. I'm gonna have you run a very simple MapReduce job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.2.1**: Follow the example on the last slide of today's lecture and run your very first MapReduce job! You should get the same output as what is shown on the slide. As your answer, insert all of the terminal output in a markdown cell below, indented by one tab (mark all of the text, hit tab so it turns orange, and hit enter).\n",
    "\n",
    ">*Hints:*\n",
    "\n",
    ">1. Create a `.py` file with the content:\n",
    "            \n",
    ">        from mrjob.job import MRJob\n",
    "> \n",
    ">        class MRWordCounter(MRJob):  \n",
    ">\n",
    ">            def mapper(self, _, line):\n",
    ">                for word in line.split():\n",
    ">                    yield word, 1\n",
    ">\n",
    ">            def reducer(self, key, values):\n",
    ">                yield key, sum(values)\n",
    ">\n",
    ">        if __name__ == '__main__':\n",
    ">            MRWordCounter.run()\n",
    "\n",
    ">2. Create a `.txt` file with the content:\n",
    ">        i am a twitter bot\n",
    ">        i am also a twitter bot\n",
    ">        wow such bot tweet also\n",
    ">        very bot like tweet also\n",
    "\n",
    ">3. Run the `.py` script with the `.txt` file as its argument in your terminal/console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crash course: *Generators*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Skip this section if you know everything about generators*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows a small, non-exhaustive, technical crash course in Python generators. By now you may have noticed that the \"functions\" inside the `mrjob` class have `yield` statement rather than the good old `return` statements that you are familiar with. What does this mean?\n",
    "\n",
    "Python functions with `yield` statements are in fact not functions at all, they are *generators*. In brief, generators are a special type of function that *yield* output everytime the interpreter reaches a `yield` statement, and then, remembering where it left off, continues from that point the next time the generator instance is called to give output.\n",
    "\n",
    "#### How generators work\n",
    "Let's define a generator called `my_generator()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T13:28:37.453144Z",
     "start_time": "2017-10-24T13:28:37.449519Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def my_generator():\n",
    "    yield \"first call\"\n",
    "    yield \"second call\"\n",
    "    yield \"final call\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an instance of this generator. We call such an instance an *iterator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T08:27:19.469579Z",
     "start_time": "2017-10-25T08:27:19.466251Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "my_iterator = my_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.next()` method, we are then going to ask our new iterator object to give us some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T08:27:19.973852Z",
     "start_time": "2017-10-25T08:27:19.969947Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first call\n"
     ]
    }
   ],
   "source": [
    "print my_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this prints the output of the first `yield` statement that we put inside the generator. Let's do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T08:27:20.614280Z",
     "start_time": "2017-10-25T08:27:20.609620Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second call\n"
     ]
    }
   ],
   "source": [
    "print my_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHA! Evidently, when calling it again, the interpreter continued from right after the first `yield`, and went straight to the next. So if we run `.next()` on it again it's surely going to give us the last string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T08:27:21.205123Z",
     "start_time": "2017-10-25T08:27:21.200992Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final call\n"
     ]
    }
   ],
   "source": [
    "print my_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Try to call `.next()` on it again and think about why it fails. You can also play around with other generators, for example try putting a `yield` statement inside a `for` loop and think about the output you get.\n",
    ">What you should realize, is that an iterator (recall: an instance of a generator) is like a tape that plays a series of `yield`s before it is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators in `mrjob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `mapper` and `reducer` are generators. Let's analyze the `mapper` generator from Ex. 19.2.1.\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word, 1\n",
    "A *line* of text runs though it, gets split into words and `yield`ed into tuples that contain each word and a one. Let's say the line is \"i am a string a short string\" and code up an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T08:32:20.061716Z",
     "start_time": "2017-10-25T08:32:20.055526Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('am', 1)\n",
      "('a', 1)\n",
      "('string', 1)\n",
      "('a', 1)\n",
      "('short', 1)\n",
      "('string', 1)\n"
     ]
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    for word in line.split():\n",
    "        yield word, 1\n",
    "\n",
    "for output in mapper(\"i am a string a short string\"):\n",
    "    print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the `self` and `_` arguments from the generator, just so that it works outside of a `class`, but what happens inside hasn't changed. Really, REALLY try to make sense of this code, because it is key to understanding how MapReduce works in Python!\n",
    ">You should understand that when you put some data into the `mapper`, out of it comes a *sequence* of key-value pairs.\n",
    "\n",
    "This entirely congruent with the figure I showed you in the lecture:\n",
    "\n",
    "<img src=\"http://ulfaslak.com/computational_analysis_of_big_data/slide_figures/mapreduce_example.png\" width=\"400\"></img>\n",
    "\n",
    "Although the arguments to the `reducer` look slightly different, it works exactly the same. It operates on one key-values pair (note that *values* is plural now) at a time, where the key is whatever you defined it to be in the `mapper` and the values are itself an iterator over all the different values that were paired with the key in the implicitly handled *group-by-key* step. The `reducer` can even have multiple `yield` statements, one for each operation that you want to make on the values. It's also possible to put the output of the `reducer` into a new `mapper` and put the output of that `mapper` into an even newer `reducer`, such as to make multi-step MapReduce operations â€“ the possibilities are endless!\n",
    "\n",
    ">Before moving on, I encourage you to play around with the script from Ex. 19.2.1, by insert some print statements here and there, deliberately breaking it, changing the output(s) of the `mapper` and the `reducer`, and so on. \n",
    "\n",
    "*Nerdy sidenote*: You could argue that the developers of `mrjob` might aswell have written their API so that the `mapper` and `reducer` were standard functions that returned a list of key-value pairs, rather than generators that produce them in sequence. The reason they chose generators is that they are much faster and have a smaller memory footprint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Some serious MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright... Time for action. In this part of today's exercises you are going to apply your freshly acquired understanding of MapReduce to do some cool highly scalable computations. All the code you write here works as well on your computer as it does on a massively parallelized Hadoop data storage system, so while you may not experience serious speedups locally, you would if the code was deployed on e.g. some of Google's data storage clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.3.1**: Modify the script from Ex. 19.2.1 so that it instead of word counts outputs the number of characters, the number of words and the numbers of lines in the input file. Post as your answer in two seperate cells, (1) the code in the script in a code cell, and (2) the terminal output in a markdown cell with the text indented by one tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.3.2**: Implement the \"Common friends\" example from today's lecture. Here is your data, you should put that into a file:\n",
    "\n",
    ">        A B C D\n",
    ">        B A C D E\n",
    ">        C A B D E\n",
    ">        D A B C E\n",
    ">        E B C D\n",
    "    \n",
    ">I removed all but the letters, but you already know that the first letter in each line is a person and the following are her friends. Report your answer in the same fashion as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.3.3**: Lets go a bit deeper. In this exercise you will implement a MapReduce-MapReduce operation, which computes the same thing as we computed above, but takes as input friend-network data in a slightly more common format: \n",
    "\n",
    ">        A B\n",
    ">        A C\n",
    ">        A D\n",
    ">        B C\n",
    ">        B D\n",
    ">        B E\n",
    ">        C D\n",
    ">        C E\n",
    ">        D E\n",
    "\n",
    ">Each line is a \"friend-link\". The links are undirected and each only occurs once.\n",
    "\n",
    ">Your job now, is to produce the same output as you did in Ex. 19.3.2, using this input data. To get started faster, use the template below, which shows how to chain together multiple MapReduce steps. Fill out the template and show the output that you get from the terminal when you run it. Clarify whether it corresponds with the output from Ex. 19.3.2.\n",
    "\n",
    ">*Hint: Try to write the first MapReduce step such that it outputs key-value pairs that correspond to the input data format from Ex. 19.3.2. Then you can reuse your solution to that exercise in the second MapReduce step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class NumberOfTriangles(MRJob):\n",
    "\n",
    "    def mapper1(self, _, line):\n",
    "        #\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        #\n",
    "\n",
    "    def mapper2(self, key, values):\n",
    "        #\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        #\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper1,\n",
    "                reducer=self.reducer1\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.mapper2,\n",
    "                reducer=self.reducer2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NumberOfTriangles.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 19.3.4**: We can go even further! Let's add a third MapReduce step to count the number of triangles in a network. Again use this input data:\n",
    "\n",
    ">        A B\n",
    ">        A C\n",
    ">        A D\n",
    ">        B C\n",
    ">        B D\n",
    ">        B E\n",
    ">        C D\n",
    ">        C E\n",
    ">        D E\n",
    "\n",
    ">to validate that your implementation works. It should produce 7 triangles.\n",
    "\n",
    ">1. Now compute the number of triangles in [this file](http://snap.stanford.edu/data/facebook_combined.txt.gz) which contains 88234 links in an anonymized facebook network. Don't print the whole output, just report the number you get.\n",
    ">2. Do the same instead using all 2766607 road segments in California as your input. Go to [this site](https://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html) and download the data in Matrix Market format (`.mtx`). Unzip the file and remove the first 50 lines from it, since that is just markup that we don't need. The file is pretty big so you can expect it to take some time (~4 minutes on my computer). Report the number you get.\n",
    "\n",
    ">*Hint: Counting triangles is equivalent to counting \"common friends\". One way to do that is to just count the collective number of common friends that exist in a network. Depending on your implementation you might want to correct your result by a factor 3, since it is likely that you end up counting each triangle three times (one for each point in it).*\n",
    "\n",
    ">*Nerdy sidenote: Why would anyone want to count triangles??? Well, in network science there is a lot of statistical measures that include the count of triangles in a network. For example, the [clustering coefficient](https://en.wikipedia.org/wiki/Clustering_coefficient), which reveals the proportion of small closed loops in a network, is computed as the number of realized triangles divided by the number of possible triangles.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
